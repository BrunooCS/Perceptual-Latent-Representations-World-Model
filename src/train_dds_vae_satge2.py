"""
This script trains the miniVAE component of the DDS+VAE model 
using masks generated by a frozen downscale network. 
It optimizes reconstruction quality and KL divergence regularization 
with free bits. Saves checkpoints periodically during training.

"""

# -------------------- Imports ----------------------

import os
import argparse
from typing import Tuple, Dict, List, Optional

import numpy as np
import torch
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm

from utils.dataset import CarRacingDataset
from utils.weights import initialize_weights
from models.dds_vae import Vision
from models.minivae.minivae import free_bits_kl_divergence


# -------------------- Training Functions ----------------------

def train_vae(
    vision: Vision,
    dataloader: DataLoader,
    optimizer: optim.Optimizer,
    scheduler: optim.lr_scheduler._LRScheduler,
    device: torch.device,
    epochs: int,
    alpha: float,
    name: str,
    path: str
) -> List[float]:
    """
    Train the miniVAE part of the Vision model while keeping other parts frozen.

    :param vision: The Vision model containing miniVAE component.
    :param dataloader: DataLoader providing the training data.
    :param optimizer: Optimizer for miniVAE training.
    :param scheduler: Learning rate scheduler.
    :param device: Computation device ('cuda' or 'cpu').
    :param epochs: Number of training epochs.
    :param alpha: Weight for KL divergence loss term.
    :param name: Model name for saving checkpoints.
    :param path: Directory to save checkpoints.
    :return: List of loss values recorded during training.
    """
    vision.mini_vae.train()
    loss_history  = []
    save_interval = max(1, int(len(dataloader) * 0.1))     # Save every 10% of an epoch

    print(f'Training miniVAE: {name}')
    for epoch in range(epochs):
        epoch_loss = 0.0
        cnt        = 0

        # 1) Train over the epoch
        with tqdm(dataloader, desc=f'Epoch {epoch + 1}/{epochs}') as pbar:
            for images, *_ in pbar:
                x = images.to(device)

                with torch.no_grad():                         # Freeze downscale network
                    mini_mask, _, _ = vision.downscale(x)

                optimizer.zero_grad()
                mini_mask_hat, mu, logvar, _ = vision.mini_vae(mini_mask)

                recon_loss = vision.perceptual_loss_upscaling(mini_mask_hat, mini_mask)
                kl_loss    = free_bits_kl_divergence(mu, logvar, free_bits=1e-2, alpha=alpha)
                loss       = recon_loss + kl_loss

                loss.backward()
                optimizer.step()
                scheduler.step()

                epoch_loss += loss.item()
                loss_history.append(loss.item())
                cnt += 1

                # Update progress bar
                current_lr = optimizer.param_groups[0]['lr']
                pbar.set_postfix(
                    recon_loss=f"{recon_loss.item():.6f}",
                    kl_loss=f"{kl_loss.item():.6f}",
                    lr=f"{current_lr:.6f}"
                )

                # Save checkpoint periodically
                if cnt % save_interval == 0:
                    save_checkpoint(vision, path, name)

        avg_loss = epoch_loss / len(dataloader)
        print(f'Epoch {epoch + 1}/{epochs} | Avg Loss: {avg_loss:.6f}')

    # Save final model
    save_checkpoint(vision, path, name)
    print(f"Training completed. Model saved to {path}/{name}.pth")

    return loss_history


def save_checkpoint(model: torch.nn.Module, path: str, name: str) -> None:
    """
    Save model checkpoint to disk.

    :param model: The model to save.
    :param path: Directory to save checkpoint.
    :param name: Checkpoint filename (without extension).
    :return: None
    """
    os.makedirs(path, exist_ok=True)
    torch.save(model.state_dict(), os.path.join(path, f'{name}.pth'))


# -------------------- Main Training Workflow ----------------------

def main(
    mask: float,
    base_ch: int,
    dataset_name: str,
    epochs: int,
    alpha: float,
    batch_size: int,
    path: str
) -> None:
    """
    Main function for setting up and training the miniVAE inside Vision model.

    :param mask: Masking percentage for feature selection (0.0-1.0).
    :param base_ch: Base number of channels in the model.
    :param dataset_name: Path to the dataset file.
    :param epochs: Number of training epochs.
    :param alpha: KL divergence loss weight.
    :param batch_size: Batch size for training.
    :param path: Directory to save model checkpoints.
    :return: None
    """
    # 1) Setup device and data
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    dataset = CarRacingDataset(dataset_name)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )

    # 2) Configure model
    model_name       = f'vision_{int(mask * 100):02d}_miniVAE'
    pretrained_name  = f'vision_{int(mask * 100):02d}.pth'
    pretrained_path  = os.path.join(path, pretrained_name)

    vision = Vision(n_features_to_select=mask, in_ch=3,
            out_ch=3, base_ch=base_ch, alpha=1.0, delta=0.1
    ).to(device)

    if os.path.exists(pretrained_path):
        vision.load_state_dict(torch.load(pretrained_path, map_location=device, weights_only=True))
        print(f"Loaded pre-trained Vision model from {pretrained_path}")
        vision.eval()
    else:
        print(f"No pre-trained model found at {pretrained_path}. Starting from scratch.")

    # 3) Freeze all except miniVAE
    for name, param in vision.named_parameters():
        param.requires_grad = 'mini_vae' in name

    # 4) Setup optimizer and scheduler
    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, vision.parameters()), lr=1e-3)
    total_steps = epochs * len(dataloader)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=1e-4)

    # 5) Initialize miniVAE weights
    initialize_weights(vision.mini_vae)

    # 6) Start training
    train_vae(vision, dataloader, optimizer, scheduler, device, epochs, alpha, model_name, path)


# ---------------------------- Argument Parser ----------------------------

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Train miniVAE inside Vision model")
    parser.add_argument("--mask", type=float, default=0.03, help="Masking percentage (0.0-1.0)")
    parser.add_argument("--base_ch", type=int, default=16, help="Base number of channels")
    parser.add_argument("--dataset_name", type=str, default='car_racing_data_10000.h5', help="Dataset filename")
    parser.add_argument("--epochs", type=int, default=4, help="Number of training epochs")
    parser.add_argument("--alpha", type=float, default=1.0, help="KL divergence loss weight")
    parser.add_argument("--batch_size", type=int, default=128, help="Training batch size")
    parser.add_argument("--path", type=str, default='trained_models', help="Path to save models")

    args = parser.parse_args()

    main(
        mask=args.mask,
        base_ch=args.base_ch,
        dataset_name=args.dataset_name,
        epochs=args.epochs,
        alpha=args.alpha,
        batch_size=args.batch_size,
        path=args.path
    )
